{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pycuda'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m perf_counter\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpycuda\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautoinit\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpycuda\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdriver\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcuda\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pycuda'"
     ]
    }
   ],
   "source": [
    "import nemo\n",
    "from nemo.collections.tts.models import FastPitchModel\n",
    "from nemo.collections.tts.models import HifiGanModel\n",
    "import tensorrt as trt\n",
    "import torch\n",
    "import onnx\n",
    "from loguru import logger\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "from time import perf_counter\n",
    "import pycuda.autoinit\n",
    "import pycuda.driver as cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-11-03 14:53:52 cloud:58] Found existing object /home/antixk/.cache/torch/NeMo/NeMo_1.23.0/tts_en_fastpitch_align/b7d086a07b5126c12d5077d9a641a38c/tts_en_fastpitch_align.nemo.\n",
      "[NeMo I 2024-11-03 14:53:52 cloud:64] Re-using file from: /home/antixk/.cache/torch/NeMo/NeMo_1.23.0/tts_en_fastpitch_align/b7d086a07b5126c12d5077d9a641a38c/tts_en_fastpitch_align.nemo\n",
      "[NeMo I 2024-11-03 14:53:52 common:924] Instantiating model from pre-trained checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " NeMo-text-processing :: INFO     :: Creating ClassifyFst grammars.\n",
      "[NeMo W 2024-11-03 14:54:07 en_us_arpabet:66] apply_to_oov_word=None, This means that some of words will remain unchanged if they are not handled by any of the rules in self.parse_one_word(). This may be intended if phonemes and chars are both valid inputs, otherwise, you may see unexpected deletions in your input.\n",
      "[NeMo W 2024-11-03 14:54:07 modelPT:165] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    dataset:\n",
      "      _target_: nemo.collections.tts.torch.data.TTSDataset\n",
      "      manifest_filepath: /ws/LJSpeech/nvidia_ljspeech_train_clean_ngc.json\n",
      "      sample_rate: 22050\n",
      "      sup_data_path: /raid/LJSpeech/supplementary\n",
      "      sup_data_types:\n",
      "      - align_prior_matrix\n",
      "      - pitch\n",
      "      n_fft: 1024\n",
      "      win_length: 1024\n",
      "      hop_length: 256\n",
      "      window: hann\n",
      "      n_mels: 80\n",
      "      lowfreq: 0\n",
      "      highfreq: 8000\n",
      "      max_duration: null\n",
      "      min_duration: 0.1\n",
      "      ignore_file: null\n",
      "      trim: false\n",
      "      pitch_fmin: 65.40639132514966\n",
      "      pitch_fmax: 2093.004522404789\n",
      "      pitch_norm: true\n",
      "      pitch_mean: 212.35873413085938\n",
      "      pitch_std: 68.52806091308594\n",
      "      use_beta_binomial_interpolator: true\n",
      "    dataloader_params:\n",
      "      drop_last: false\n",
      "      shuffle: true\n",
      "      batch_size: 24\n",
      "      num_workers: 0\n",
      "    \n",
      "[NeMo W 2024-11-03 14:54:07 modelPT:172] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    dataset:\n",
      "      _target_: nemo.collections.tts.torch.data.TTSDataset\n",
      "      manifest_filepath: /ws/LJSpeech/nvidia_ljspeech_val_clean_ngc.json\n",
      "      sample_rate: 22050\n",
      "      sup_data_path: /raid/LJSpeech/supplementary\n",
      "      sup_data_types:\n",
      "      - align_prior_matrix\n",
      "      - pitch\n",
      "      n_fft: 1024\n",
      "      win_length: 1024\n",
      "      hop_length: 256\n",
      "      window: hann\n",
      "      n_mels: 80\n",
      "      lowfreq: 0\n",
      "      highfreq: 8000\n",
      "      max_duration: null\n",
      "      min_duration: null\n",
      "      ignore_file: null\n",
      "      trim: false\n",
      "      pitch_fmin: 65.40639132514966\n",
      "      pitch_fmax: 2093.004522404789\n",
      "      pitch_norm: true\n",
      "      pitch_mean: 212.35873413085938\n",
      "      pitch_std: 68.52806091308594\n",
      "      use_beta_binomial_interpolator: true\n",
      "    dataloader_params:\n",
      "      drop_last: false\n",
      "      shuffle: false\n",
      "      batch_size: 24\n",
      "      num_workers: 0\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-11-03 14:54:07 features:289] PADDING: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-11-03 14:54:07 nemo_logging:349] /home/antixk/miniconda3/envs/trt/lib/python3.10/site-packages/nemo/core/connectors/save_restore_connector.py:571: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "      return torch.load(model_weights, map_location='cpu')\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-11-03 14:54:07 save_restore_connector:249] Model FastPitchModel was successfully restored from /home/antixk/.cache/torch/NeMo/NeMo_1.23.0/tts_en_fastpitch_align/b7d086a07b5126c12d5077d9a641a38c/tts_en_fastpitch_align.nemo.\n",
      "[NeMo I 2024-11-03 14:54:07 cloud:58] Found existing object /home/antixk/.cache/torch/NeMo/NeMo_1.23.0/tts_en_hifitts_hifigan_ft_fastpitch/dcbdefa79d5e3587c93636d4f53260b8/tts_en_hifitts_hifigan_ft_fastpitch.nemo.\n",
      "[NeMo I 2024-11-03 14:54:07 cloud:64] Re-using file from: /home/antixk/.cache/torch/NeMo/NeMo_1.23.0/tts_en_hifitts_hifigan_ft_fastpitch/dcbdefa79d5e3587c93636d4f53260b8/tts_en_hifitts_hifigan_ft_fastpitch.nemo\n",
      "[NeMo I 2024-11-03 14:54:07 common:924] Instantiating model from pre-trained checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-11-03 14:54:07 modelPT:165] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    dataset:\n",
      "      _target_: nemo.collections.tts.torch.data.VocoderDataset\n",
      "      manifest_filepath: /manifests/hifitts_fp_1000epoch_mels_train.json\n",
      "      sample_rate: 44100\n",
      "      n_segments: 16384\n",
      "      max_duration: null\n",
      "      min_duration: 0.75\n",
      "      load_precomputed_mel: true\n",
      "      hop_length: 512\n",
      "    dataloader_params:\n",
      "      drop_last: false\n",
      "      shuffle: true\n",
      "      batch_size: 100\n",
      "      num_workers: 4\n",
      "    \n",
      "[NeMo W 2024-11-03 14:54:07 modelPT:172] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    dataset:\n",
      "      _target_: nemo.collections.tts.torch.data.VocoderDataset\n",
      "      manifest_filepath: /manifests/hifitts_fp_1000epoch_mels_test.json\n",
      "      sample_rate: 44100\n",
      "      n_segments: 131072\n",
      "      max_duration: null\n",
      "      min_duration: 3\n",
      "      load_precomputed_mel: true\n",
      "      hop_length: 512\n",
      "    dataloader_params:\n",
      "      drop_last: false\n",
      "      shuffle: false\n",
      "      batch_size: 16\n",
      "      num_workers: 4\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-11-03 14:54:07 features:289] PADDING: 0\n",
      "[NeMo I 2024-11-03 14:54:07 features:297] STFT using exact pad\n",
      "[NeMo I 2024-11-03 14:54:07 features:289] PADDING: 0\n",
      "[NeMo I 2024-11-03 14:54:07 features:297] STFT using exact pad\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-11-03 14:54:07 nemo_logging:349] /home/antixk/miniconda3/envs/trt/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "      WeightNorm.apply(module, name, dim)\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-11-03 14:54:08 save_restore_connector:249] Model HifiGanModel was successfully restored from /home/antixk/.cache/torch/NeMo/NeMo_1.23.0/tts_en_hifitts_hifigan_ft_fastpitch/dcbdefa79d5e3587c93636d4f53260b8/tts_en_hifitts_hifigan_ft_fastpitch.nemo.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "fastpitch = FastPitchModel.from_pretrained(\"tts_en_fastpitch\").eval().cuda()\n",
    "hifigan = HifiGanModel.from_pretrained(\"tts_en_hifitts_hifigan_ft_fastpitch\").eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(spec_gen_model, vocoder_model, str_input):\n",
    "    parser_model = spec_gen_model\n",
    "    with torch.no_grad():\n",
    "        parsed = parser_model.parse(str_input)\n",
    "        gen_spec_kwargs = {}\n",
    "        \n",
    "        \n",
    "        spectrogram = spec_gen_model.generate_spectrogram(tokens=parsed, **gen_spec_kwargs)\n",
    "        audio = vocoder_model.convert_spectrogram_to_audio(spec=spectrogram)\n",
    "        \n",
    "        if audio_generator == \"hifigan\":\n",
    "            audio = vocoder_model._bias_denoise(audio, spectrogram).squeeze(1)\n",
    "    if spectrogram is not None:\n",
    "        if isinstance(spectrogram, torch.Tensor):\n",
    "            spectrogram = spectrogram.to('cpu').numpy()\n",
    "        if len(spectrogram.shape) == 3:\n",
    "            spectrogram = spectrogram[0]\n",
    "    if isinstance(audio, torch.Tensor):\n",
    "        audio = audio.to('cpu').numpy()\n",
    "    return spectrogram, audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-11-03 15:04:53 nemo_logging:349] /home/antixk/miniconda3/envs/trt/lib/python3.10/site-packages/torch/onnx/utils.py:1963: UserWarning: Provided key mel for dynamic axes is not a valid input/output name\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2024-11-03 15:04:53 nemo_logging:349] /home/antixk/miniconda3/envs/trt/lib/python3.10/site-packages/torch/onnx/_internal/jit_utils.py:308: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ../torch/csrc/jit/passes/onnx/constant_fold.cpp:178.)\n",
      "      _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n",
      "    \n",
      "[NeMo W 2024-11-03 15:04:53 nemo_logging:349] /home/antixk/miniconda3/envs/trt/lib/python3.10/site-packages/torch/onnx/utils.py:663: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ../torch/csrc/jit/passes/onnx/constant_fold.cpp:178.)\n",
      "      _C._jit_pass_onnx_graph_shape_type_inference(\n",
      "    \n",
      "[NeMo W 2024-11-03 15:04:53 nemo_logging:349] /home/antixk/miniconda3/envs/trt/lib/python3.10/site-packages/torch/onnx/utils.py:1186: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ../torch/csrc/jit/passes/onnx/constant_fold.cpp:178.)\n",
      "      _C._jit_pass_onnx_graph_shape_type_inference(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-11-03 15:04:54 exportable:131] Successfully exported FastPitchModel to pretrained_models/fastpitch.onnx\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['pretrained_models/fastpitch.onnx'],\n",
       " ['nemo.collections.tts.models.fastpitch.FastPitchModel exported to ExportFormat.ONNX'])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "fastpitch.export(\"pretrained_models/fastpitch.onnx\",\n",
    "                 input_example=fastpitch.input_example(),\n",
    "                 onnx_opset_version=19, \n",
    "                 dynamic_axes={'text' : {0 : 'batch_size',\n",
    "                                         1 : \"seq_length\",},\n",
    "                                'pitch':{0:'batch_size', 1:'pitch_length'},\n",
    "                                'pace':{0:'batch_size', 1:'pace_length'},\n",
    "                                'mel': {0:'batch_size', \n",
    "                                        2: 'mel_length'}},)\n",
    "                #  verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing weight norm...\n",
      "Removing weight norm...\n",
      "[NeMo I 2024-11-03 15:43:32 exportable:131] Successfully exported HifiGanModel to pretrained_models/hifigan.onnx\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['pretrained_models/hifigan.onnx'],\n",
       " ['nemo.collections.tts.models.hifigan.HifiGanModel exported to ExportFormat.ONNX'])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "hifigan.export(\"pretrained_models/hifigan.onnx\",\n",
    "                input_example=hifigan.input_example(),\n",
    "                onnx_opset_version=19, \n",
    "                dynamic_axes={'spec': {0:'batch_size', \n",
    "                                      2: 'mel_length'}},)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-03 14:52:00.219\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m12\u001b[0m - \u001b[1mTensorRT Version: 10.6.0\u001b[0m\n",
      "\u001b[32m2024-11-03 14:52:00.221\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mParsing ONNX file.\u001b[0m\n",
      "\u001b[32m2024-11-03 14:52:00.273\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mSetting optimization profile.\u001b[0m\n",
      "\u001b[32m2024-11-03 14:52:00.274\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m39\u001b[0m - \u001b[1mBuilding TensorRT engine. This may take a few minutes.\u001b[0m\n",
      "\u001b[32m2024-11-03 14:52:22.926\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m44\u001b[0m - \u001b[1mTensorRT engine saved to pretrained_models/hifigan.plan\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "trt.init_libnvinfer_plugins(TRT_LOGGER, namespace=\"\")\n",
    "\n",
    "onnxfile = \"pretrained_models/hifigan.onnx\"\n",
    "# Sanity check\n",
    "onnx_model = onnx.load(onnxfile)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "del onnx_model\n",
    "\n",
    "major, minor, patch = trt.__version__.split('.')\n",
    "\n",
    "logger.info(f\"TensorRT Version: {major}.{minor}.{patch}\")\n",
    "EXPLICIT_BATCH = 1 << (int)(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH) #we have enabled the explicit Batch\n",
    "\n",
    "\n",
    "with (trt.Builder(TRT_LOGGER) as builder, \n",
    "      builder.create_network() as network, \n",
    "      trt.OnnxParser(network, TRT_LOGGER) as parser, \n",
    "      builder.create_builder_config() as builder_config):\n",
    "    builder_config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 1<< 30)\n",
    "\n",
    "    # if args.use_amp:\n",
    "    #     logger.info(\"Using FP16 Precision\")\n",
    "    #     builder_config.set_flag(trt.BuilderFlag.FP16)\n",
    "\n",
    "    logger.info(\"Parsing ONNX file.\")\n",
    "    with open(onnxfile, 'rb') as model:\n",
    "        if not parser.parse(model.read()):\n",
    "            for error in range(parser.num_errors):\n",
    "                logger.error(parser.get_error(error))\n",
    "            logger.error(\"Failed to parse ONNX file.\")\n",
    "            raise Exception(\"Failed to parse ONNX file.\")\n",
    "\n",
    "    # Set dynamic shapes\n",
    "    logger.info(\"Setting optimization profile.\")\n",
    "    profile = builder.create_optimization_profile()\n",
    "    # TODO: Set proper dynamic shapes\n",
    "    profile.set_shape(\"mel\", (1, 80, 661), (4, 80, 661), (8, 80,661))\n",
    "    builder_config.add_optimization_profile(profile)\n",
    "    builder_config.default_device_type = trt.DeviceType.GPU\n",
    "\n",
    "    logger.info(\"Building TensorRT engine. This may take a few minutes.\")\n",
    "    serialized_engine = builder.build_serialized_network(network, builder_config)\n",
    "\n",
    "    with open('pretrained_models/hifigan.plan', 'wb') as f:\n",
    "        f.write(serialized_engine)\n",
    "        logger.info(\"TensorRT engine saved to pretrained_models/hifigan.plan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m [(k, v\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m fastpitch\u001b[38;5;241m.\u001b[39minput_example()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mitems()]\n\u001b[0;32m----> 3\u001b[0m \u001b[43mfastpitch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "[(k, v.shape) for k,v in fastpitch.input_example()[0].items()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 24)\n",
      "(1, 80, 135)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2024-11-03 15:39:23.576740519 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 5 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.\u001b[m\n",
      "\u001b[0;93m2024-11-03 15:39:23.579105394 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2024-11-03 15:39:23.579112703 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    }
   ],
   "source": [
    "# Create ONNXRuntime Session for FastPitch\n",
    "options = ort.SessionOptions()\n",
    "options.enable_profiling=False\n",
    "# options.log_severity=0# options.log_verbosity_level0\n",
    "\n",
    "\n",
    "ort_session = ort.InferenceSession( 'pretrained_models/fastpitch.onnx',\n",
    "    sess_options=options,\n",
    "    providers=[  'CUDAExecutionProvider'])\n",
    "\n",
    "# print(dir(ort_session))\n",
    "# print(ort_session.get_inputs())\n",
    "# print(ort_session.get_outputs())\n",
    "io_binding = ort_session.io_binding()\n",
    "\n",
    "encoded_text = fastpitch.parse(\"Yo,waddup!, how are you?\").detach().cpu().numpy()\n",
    "\n",
    "print(encoded_text.shape)\n",
    "pace = np.array([[1.0]], dtype=np.float32)\n",
    "pitch = np.array([[1.0]], dtype=np.float32)\n",
    "\n",
    "encoded_text = ort.OrtValue.ortvalue_from_numpy(encoded_text)\n",
    "pace = ort.OrtValue.ortvalue_from_numpy(pace)\n",
    "pitch = ort.OrtValue.ortvalue_from_numpy(pitch)\n",
    "\n",
    "# ortvalue.device_name()  # 'cpu'\n",
    "# ortvalue.shape()        # shape of the numpy array X\n",
    "# ortvalue.data_type()    # 'tensor(float)'\n",
    "# ortvalue.is_tensor()    # 'True'\n",
    "# np.array_equal(ortvalue.numpy(), X)  # 'True'\n",
    "\n",
    "results = ort_session.run(None, input_feed= {\"text\": encoded_text, \"pitch\":pitch, \"pace\": pace})\n",
    "\n",
    "mel = results[0]\n",
    "\n",
    "print(mel.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HostDeviceMem(object):\n",
    "    def __init__(self, host_mem, device_mem):\n",
    "        \"\"\"Within this context, host_mom means the cpu memory and device means the GPU memory\"\"\"\n",
    "        self.host = host_mem\n",
    "        self.device = device_mem\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Host:\\n\" + str(self.host) + \"\\nDevice:\\n\" + str(self.device)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "    \n",
    "with open(\"pretrained_models/hifigan.plan\", \"rb\") as f:\n",
    "    serialized_engine = f.read()\n",
    "\n",
    "engine = runtime.deserialize_cuda_engine(serialized_engine)\n",
    "context = engine.create_execution_context()\n",
    "inputs, outputs, bindings = [], [], []\n",
    "stream = cuda.Stream()\n",
    "\n",
    "for i in range(engine.num_io_tensors):\n",
    "    tensor_name = engine.get_tensor_name(i)\n",
    "    logger.info(f\"Tensor:{tensor_name}, Shape:{engine.get_tensor_shape(tensor_name)}\")\n",
    "\n",
    "\n",
    "# class\n",
    "\n",
    "def infer(mel: np.array):\n",
    "    # https://github.com/NVIDIA/TensorRT/issues/4230\n",
    "    # Actual shapes of the inputs\n",
    "    input_shapes = mel.shape\n",
    "\n",
    "    B = input_shapes[0]\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    bindings = []\n",
    "    context.set_input_shape(\"mel\", mel.shape)\n",
    "\n",
    "    for i in range(engine.num_io_tensors):\n",
    "        tensor_name = engine.get_tensor_name(i)\n",
    "        dtype = trt.nptype(engine.get_tensor_dtype(tensor_name))\n",
    "\n",
    "        # Check if it's an input or output tensor\n",
    "        if engine.get_tensor_mode(tensor_name) == trt.TensorIOMode.INPUT:\n",
    "            shape = input_shapes  # Get the shape from the input shapes\n",
    "\n",
    "            size = trt.volume(shape)\n",
    "            host_mem = cuda.pagelocked_empty(size, dtype)\n",
    "            device_mem = cuda.mem_alloc(host_mem.nbytes)\n",
    "            inputs.append(HostDeviceMem(host_mem, device_mem))\n",
    "            bindings.append(int(device_mem))\n",
    "            np.copyto(inputs[-1].host, locals()[tensor_name].ravel())\n",
    "        else:\n",
    "            temp_shape = (B, *engine.get_tensor_shape(tensor_name)[1:])\n",
    "            # temp_shape = (1,)  # Placeholder, adjust if necessary\n",
    "            size = trt.volume(temp_shape)\n",
    "            # print(temp_shape, size, dtype)\n",
    "            host_mem = cuda.pagelocked_empty(size, dtype)\n",
    "            device_mem = cuda.mem_alloc(host_mem.nbytes)\n",
    "            outputs.append(HostDeviceMem(host_mem, device_mem))\n",
    "            bindings.append(int(device_mem))\n",
    "\n",
    "    # Transfer inputs to device\n",
    "    for i in range(len(inputs)):\n",
    "        cuda.memcpy_htod_async(inputs[i].device, inputs[i].host, stream)\n",
    "\n",
    "    # Set tensor address for each input/output\n",
    "    for i in range(engine.num_io_tensors):\n",
    "        context.set_tensor_address(engine.get_tensor_name(i), bindings[i])\n",
    "\n",
    "    context.execute_async_v3(stream.handle)\n",
    "\n",
    "    # Transfer predictions back\n",
    "    cuda.memcpy_dtoh_async(outputs[0].host, outputs[0].device, stream)\n",
    "\n",
    "    # Synchronize the stream\n",
    "    stream.synchronize()\n",
    "\n",
    "    return outputs[0].host\n",
    "\n",
    "def cleanup():\n",
    "    for input_mem in inputs:\n",
    "        input_mem.device.free()  # Free device memory for each input\n",
    "    for output_mem in outputs:\n",
    "        output_mem.device.free()  # Free device memory for each output\n",
    "\n",
    "\n",
    "\n",
    "# Run inference\n",
    "mel = np.random.rand(2, 80, 661).astype(np.float32)\n",
    "audio = np.random.rand(1, 2, 16).astype(np.float32)\n",
    "start_time = perf_counter()\n",
    "output = infer(mel)\n",
    "end_time = perf_counter()\n",
    "# print time in milliseconds\n",
    "print(f\"Time taken:{(end_time - start_time) * 1000:.2f} ms\")\n",
    "print(\"Output shape:\", output.shape)\n",
    "\n",
    "print(output)\n",
    "\n",
    "# Clean up memory after inference\n",
    "cleanup()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
